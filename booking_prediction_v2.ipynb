{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of bookings based on user behavior\n",
    "Data Scientist â€“ User Profiling, Hotel Search\n",
    "\n",
    "- Author: Kai Chen\n",
    "- Date:   Apr, 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import operator\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "import csv\n",
    "import gc\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import matthews_corrcoef, classification_report, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import catboost\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            user_id    session_id  referer_code        is_app      agent_id  \\\n",
      "count  5.864434e+06  5.864434e+06  5.864434e+06  5.864434e+06  5.864434e+06   \n",
      "mean   4.612515e+18  4.607873e+18  1.050245e+01  1.247070e-01  7.267086e+00   \n",
      "std    2.657178e+18  2.656793e+18  2.855244e+01  3.303864e-01  3.802190e+00   \n",
      "min    3.883091e+14  1.097161e+14  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%    2.307265e+18  2.310716e+18  0.000000e+00  0.000000e+00  6.000000e+00   \n",
      "50%    4.624574e+18  4.606553e+18  1.000000e+00  0.000000e+00  9.000000e+00   \n",
      "75%    6.897454e+18  6.892477e+18  1.000000e+00  0.000000e+00  1.000000e+01   \n",
      "max    9.223267e+18  9.223359e+18  9.900000e+01  1.000000e+00  1.400000e+01   \n",
      "\n",
      "       traffic_type   has_booking     action_id     reference          step  \n",
      "count  5.864434e+06  5.864434e+06  5.864434e+06  5.864434e+06  5.864434e+06  \n",
      "mean   1.882018e+00  1.301094e-01  2.812956e+03  4.897366e+05  5.463159e+01  \n",
      "std    1.407386e+00  3.364238e-01  1.636151e+03  2.865475e+06  1.318930e+02  \n",
      "min    1.000000e+00  0.000000e+00 -1.000000e+01 -1.000000e+01  0.000000e+00  \n",
      "25%    1.000000e+00  0.000000e+00  2.119000e+03  1.000000e+00  6.000000e+00  \n",
      "50%    1.000000e+00  0.000000e+00  2.146000e+03  3.164400e+04  1.800000e+01  \n",
      "75%    2.000000e+00  0.000000e+00  2.501000e+03  1.296900e+05  4.800000e+01  \n",
      "max    1.000000e+01  1.000000e+00  8.091000e+03  6.814322e+08  3.133000e+03  \n",
      "          ymd          user_id           session_id  referer_code  is_app  \\\n",
      "0  2017-04-23  388309106223940  3052767322364990735             0       0   \n",
      "1  2017-04-10  452426828488840  1022778951418899936             0       0   \n",
      "2  2017-04-10  452426828488840  1022778951418899936             0       0   \n",
      "\n",
      "   agent_id  traffic_type  has_booking  action_id  reference  step  \n",
      "0         2             1            0       8001    1323836     1  \n",
      "1        10             2            0       2116     929835     1  \n",
      "2        10             2            0       6999          0     2  \n"
     ]
    }
   ],
   "source": [
    "train_user_df = pd.read_csv('train_user_df.csv')\n",
    "print(train_user_df.describe())\n",
    "print(train_user_df.head(3))\n",
    "\n",
    "target_user_df = pd.read_csv('target_user_df.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, group in train_user_df.groupby('session_id'):\n",
    "    nb_bookings = group[group['has_booking'] == 1].shape[0]\n",
    "    if (nb_bookings > 0 and nb_bookings != group.shape[0] and np.max(group['step']) != group.shape[0]):\n",
    "        print(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211\n",
      "121529\n"
     ]
    }
   ],
   "source": [
    "action_id_list = list(train_user_df.action_id.unique())\n",
    "print(len(action_id_list))\n",
    "\n",
    "reference_list = list(train_user_df.reference.unique())\n",
    "print(len(reference_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min nb steps: 1\n",
      "max nb steps: 2924\n",
      "std nb steps: 70.27699664977659\n"
     ]
    }
   ],
   "source": [
    "nb_steps_booking_list = []\n",
    "\n",
    "for name, group in train_user_df.groupby('session_id'):\n",
    "    nb_bookings = group[group['has_booking'] == 1].shape[0]\n",
    "    if (nb_bookings > 0):\n",
    "        nb_steps_booking_list.append(group.shape[0])\n",
    "\n",
    "print('min nb steps: {}'.format(np.min(nb_steps_booking_list)))\n",
    "print('max nb steps: {}'.format(np.max(nb_steps_booking_list)))\n",
    "print('std nb steps: {}'.format(np.std(nb_steps_booking_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Is there a common action_id in the sessions with bookings?\n",
    "def intersection(lst1, lst2):\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    return lst3\n",
    "\n",
    "inter_actions = []\n",
    "\n",
    "for name, group in train_user_df.groupby('session_id'):\n",
    "    nb_bookings = group[group['has_booking'] == 1].shape[0]\n",
    "    if (nb_bookings > 0):\n",
    "        actions = list(group['action_id'].values)\n",
    "        inter_actions = intersection(inter_actions, actions)\n",
    "    \n",
    "print(inter_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Is there a common reference in the sessions with bookings?\n",
    "def intersection(lst1, lst2):\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    return lst3\n",
    "\n",
    "inter_actions = []\n",
    "\n",
    "for name, group in train_user_df.groupby('session_id'):\n",
    "    nb_bookings = group[group['has_booking'] == 1].shape[0]\n",
    "    if (nb_bookings > 0):\n",
    "        actions = list(group['reference'].values)\n",
    "        inter_actions = intersection(inter_actions, actions)\n",
    "    \n",
    "print(inter_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "In order to predict if a session has a booking or not, it is not sufficient to take only the session information (i.e., referer_code, is_app, agent_id, traffic_type) and the action information (i.e., action_id, reference) of the last step as features. Ideally, we have to take not only the session information but also all the action information in the session as features. Due to the limitation of the computation resource, for each session, I take the last n steps action information with the session information as features. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---\n",
    "# Define file paths\n",
    "TRAIN_BOOKING_FILE_PATH = 'data/case_study_bookings_train.csv'    # training sessions for bookings\n",
    "TARGET_BOOKING_FILE_PATH = 'data/case_study_bookings_target.csv'  # target sessions to predict bookings\n",
    "\n",
    "TRAIN_ACTION_FILE_PATH = 'data/case_study_actions_train.csv'      # training set of user actions\n",
    "TARGET_ACTION_FILE_PATH = 'data/case_study_actions_target.csv'    # user actions in the target sessions\n",
    "\n",
    "# replace the NAN values by a specific value\n",
    "NA_ACTION_ID = -10\n",
    "NA_REFERENCE_ID = -10\n",
    "NA_STEP = 0\n",
    "\n",
    "# feature_columns = ['ymd', 'referer_code', 'is_app', 'agent_id', 'traffic_type', 'action_id', 'reference', 'step']\n",
    "feature_columns = ['referer_code', 'is_app', 'agent_id', 'traffic_type', 'action_id', 'reference', 'step']\n",
    "target_column   = ['has_booking']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, nb_pre_steps=1,\n",
    "                 feature_columns = ['referer_code', 'is_app', 'agent_id', 'traffic_type', 'action_id', 'reference', 'step'],\n",
    "                 # feature_columns = ['ymd', 'referer_code', 'is_app', 'agent_id', 'traffic_type', 'action_id', 'reference', 'step'],\n",
    "                 previous_action_names=['action_id', 'reference'],\n",
    "                 target_column = 'has_booking',\n",
    "                 default_action_values = [-10, -10]):\n",
    "    \"\"\"\n",
    "    Create a dataframe, such that each row contains the information of a session.\n",
    "    Since for each session, there is a sequence of information.\n",
    "    In this dataframe, for each session,\n",
    "    I take only the last n steps information with its nb_pre_steps number of previous steps information.\n",
    "    \"\"\"\n",
    "    print('\\n== prepare data ==')\n",
    "\n",
    "    total_nb_rows = len(df['session_id'].unique())\n",
    "\n",
    "    # initialize the column names\n",
    "    # columns_add = ['duration'] # add new features\n",
    "    columns_add = [f_name for f_name in feature_columns]\n",
    "\n",
    "    for i in range(0, nb_pre_steps):\n",
    "        for previous_action_name in previous_action_names:\n",
    "            col_name = '{}_{}'.format(previous_action_name, (i+1))\n",
    "            columns_add.append(col_name)\n",
    "\n",
    "    if target_column in df.columns:\n",
    "        columns_add.append(target_column)\n",
    "\n",
    "    df_new = pd.DataFrame(columns=columns_add)\n",
    "\n",
    "    start_time = time.time()\n",
    "    index = 0 # index of each row\n",
    "    for name, group in df.groupby('session_id'):\n",
    "        max_step = np.max(group['step'])\n",
    "\n",
    "        # get start time\n",
    "        # min_step = np.min(group['step'])\n",
    "        # start_time = pd.to_datetime(group[group['step'] == max_step]['ymd'].values[0].astype('str'))\n",
    "        # get end time\n",
    "        # end_time = pd.to_datetime(group[group['step'] == min_step]['ymd'].values[0].astype('str'))\n",
    "        # compute the duration of the session\n",
    "        # duration = (end_time-start_time).total_seconds()\n",
    "\n",
    "        # for each session, get its information in the last step\n",
    "        sub_df = group[group['step'] == max_step]\n",
    "\n",
    "        # set the initial values of this session\n",
    "        val_add = []\n",
    "\n",
    "        # duration\n",
    "        # val_add.append(duration)\n",
    "\n",
    "        for feature_column in feature_columns:\n",
    "            val_add.append(sub_df[feature_column].values[0])\n",
    "\n",
    "        for i in range(0, nb_pre_steps):\n",
    "            for j, previous_action_name in enumerate(previous_action_names):\n",
    "                val_add.append(default_action_values[j])\n",
    "\n",
    "        if target_column in sub_df.columns:\n",
    "            val_add.append(sub_df[target_column].values[0])\n",
    "\n",
    "        df_new = df_new.append(pd.DataFrame([val_add], columns=columns_add))\n",
    "\n",
    "        # get the session previous steps information and add it to the new row\n",
    "        for i in range(0, nb_pre_steps):\n",
    "            step = max_step - i - 1\n",
    "            sub_df = group[group['step'] == step]\n",
    "            if (not sub_df is None) and (not sub_df.empty):\n",
    "                for previous_action_name in previous_action_names:\n",
    "                    col_name = '{}_{}'.format(previous_action_name, step)\n",
    "                    # print('previous')\n",
    "                    # print(previous_df[previous_action].values)\n",
    "                    # print('----')\n",
    "                    df_new.iloc[index][col_name] = sub_df[previous_action_name].values[0]\n",
    "\n",
    "\n",
    "        index += 1\n",
    "\n",
    "        if index % 20000 == 0:\n",
    "            time_used = time.time() - start_time\n",
    "            time_needed = time_used / index * (total_nb_rows-index)\n",
    "            print('\\n{} / {}'.format(index, total_nb_rows))\n",
    "            print('time used (mins): {}'.format(round(time_used / 60)))\n",
    "            print('time needed (mins): {}'.format(round(time_needed / 60)))\n",
    "            print(df_new.iloc[random.randint(0, index-1)][columns_add])\n",
    "            if target_column in df_new.columns:\n",
    "                print('{}: {}'.format(target_column, df_new.iloc[random.randint(0, index-1)][target_column]))\n",
    "\n",
    "    return df_new\n",
    "\n",
    "def prepare_datasets(param_dict):\n",
    "    train_user_df = param_dict['train']\n",
    "    target_user_df = param_dict['target']\n",
    "    nb_prev_step = param_dict['nb_prev_step']\n",
    "\n",
    "    print('\\n{}'.format(nb_prev_step))\n",
    "\n",
    "    train_user_df_new = prepare_data(train_user_df, nb_pre_steps=nb_prev_step)\n",
    "    df_path = '{}-{}.csv'.format('train_user_df', nb_prev_step)\n",
    "    train_user_df_new.to_csv(df_path, index=False)\n",
    "    print('\\nsave train dataframe to {}'.format(df_path))\n",
    "    print(train_user_df_new.head(2))\n",
    "\n",
    "    target_user_df_new = prepare_data(target_user_df, nb_pre_steps=nb_prev_step)\n",
    "    df_path = '{}-{}.csv'.format('target_user_df', nb_prev_step)\n",
    "    target_user_df_new.to_csv(df_path, index=False)\n",
    "    print('\\nsave test dataframe to {}'.format(df_path))\n",
    "    print(target_user_df_new.head(2))\n",
    "\n",
    "    del train_user_df_new\n",
    "    del target_user_df_new\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------\n",
    "# read train data\n",
    "train_booking_df = pd.read_csv(TRAIN_BOOKING_FILE_PATH, sep='\\t')\n",
    "train_booking_df['ymd'] = pd.to_datetime(train_booking_df['ymd'].astype('str'))\n",
    "train_action_df = pd.read_csv(TRAIN_ACTION_FILE_PATH, sep='\\t')\n",
    "train_action_df['ymd'] = pd.to_datetime(train_action_df['ymd'].astype('str'))\n",
    "train_user_df = pd.merge(train_booking_df, train_action_df, on=['ymd', 'user_id', 'session_id'], how='left')\n",
    "train_user_df = preprocessing(train_user_df)\n",
    "train_user_df.to_csv('train_user_df.csv')\n",
    "\n",
    "# ----------\n",
    "# read test data\n",
    "target_booking_df = pd.read_csv(TARGET_BOOKING_FILE_PATH, sep='\\t')\n",
    "target_booking_df['ymd'] = pd.to_datetime(target_booking_df['ymd'].astype('str'))\n",
    "target_action_df = pd.read_csv(TARGET_ACTION_FILE_PATH, sep='\\t')\n",
    "target_action_df['ymd'] = pd.to_datetime(target_action_df['ymd'].astype('str'))\n",
    "target_user_df = pd.merge(target_booking_df, target_action_df, on=['ymd', 'user_id', 'session_id'], how='left')\n",
    "target_user_df = preprocessing(target_user_df)\n",
    "target_user_df.to_csv('target_user_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user_df = pd.read_csv('train_user_df.csv')\n",
    "# train_user_df['ymd'] = pd.to_datetime(train_user_df['ymd'].astype('str'))\n",
    "target_user_df = pd.read_csv('target_user_df.csv')\n",
    "# target_user_df['ymd'] = pd.to_datetime(target_user_df['ymd'].astype('str'))\n",
    "\n",
    "# print(train_user_df.shape)\n",
    "# print(train_user_df.head(3))\n",
    "# print(target_user_df.head(3))\n",
    "\n",
    "\n",
    "# nb_prev_step_list = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
    "nb_prev_step_list = [32]\n",
    "param_dict_list = []\n",
    "for nb_prev_step in nb_prev_step_list:\n",
    "    param_dict = dict()\n",
    "    param_dict['train'] = train_user_df\n",
    "    param_dict['target'] = target_user_df\n",
    "    param_dict['nb_prev_step'] = nb_prev_step\n",
    "    param_dict_list.append(param_dict)\n",
    "\n",
    "n_jobs = 1\n",
    "with Pool(n_jobs) as p:\n",
    "    p.map(prepare_datasets, param_dict_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['referer_code', 'is_app', 'agent_id', 'traffic_type', 'action_id', 'reference', 'step']\n",
    "target_column = ['has_booking']\n",
    "\n",
    "\n",
    "def get_train_set(df, feature_columns, target_column):\n",
    "    print('\\n === get train set === \\n')\n",
    "\n",
    "    train_df = df[feature_columns + target_column]\n",
    "\n",
    "    train_x = train_df[feature_columns]\n",
    "    train_y = train_df[target_column].values\n",
    "\n",
    "    # https://stackoverflow.com/questions/31995175/scikit-learn-cross-val-score-too-many-indices-for-array\n",
    "    \"\"\"\n",
    "    When we do cross validation in scikit-learn, the process requires an (R,) shape label instead of (R,1). \n",
    "    Although they are the same thing to some extend, their indexing mechanisms are different. So in your case, just add:\n",
    "    c, r = labels.shape\n",
    "    labels = labels.reshape(c,)\n",
    "    \"\"\"\n",
    "    c, r = train_y.shape\n",
    "    train_y = train_y.reshape(c, )\n",
    "\n",
    "    return train_x, train_y\n",
    "\n",
    "\n",
    "def get_test_set(df, feature_columns):\n",
    "    print('\\n === get test set === \\n')\n",
    "\n",
    "    test_x = df[feature_columns]\n",
    "\n",
    "    return test_x\n",
    "\n",
    "\n",
    "def timer(start_time=None):\n",
    "    # fork from https://www.kaggle.com/tilii7/hyperparameter-grid-search-with-xgboost\n",
    "    if not start_time:\n",
    "        start_time = datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))\n",
    "\n",
    "\n",
    "def train_xgb(X_train, Y_train, hyperparameter_tuning=False, model_path=None, n_jobs=3, folds=3, param_comb=5, n_estimators=100):\n",
    "    \"\"\"\n",
    "    train a xgb model\n",
    "\n",
    "    Reference\n",
    "    https://www.kaggle.com/tilii7/hyperparameter-grid-search-with-xgboost\n",
    "    \"\"\"\n",
    "\n",
    "    # xgb_clf = XGBClassifier(learning_rate=0.01,\n",
    "    #                     n_estimators=200,\n",
    "    #                     objective='binary:logistic',\n",
    "    #                     silent=True, nthread=nthread)\n",
    "\n",
    "    print('\\n === train a xgb model === \\n')\n",
    "\n",
    "    xgb_clf = XGBClassifier(n_estimators=n_estimators, nthread=n_jobs, objective='binary:logistic', silent=True,)\n",
    "\n",
    "    if hyperparameter_tuning:\n",
    "        print('xgb hyperparameter tuning ...')\n",
    "\n",
    "        params = {\n",
    "            'n_estimators': [5, 10, 80, 100, 200],\n",
    "            'min_child_weight': [1, 5, 10],\n",
    "            # 'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "            'gamma': [0.5, 1, 1.5, 2],\n",
    "            # 'subsample': [0.6, 0.8, 1.0],\n",
    "            'subsample': [0.6, 0.8, 1],\n",
    "            # 'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "            'colsample_bytree': [0.6, 0.8, 1],\n",
    "            # 'max_depth': [3, 4, 5]\n",
    "            'max_depth': [2, 4, 6],\n",
    "        }\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "\n",
    "        random_search = RandomizedSearchCV(xgb_clf, param_distributions=params, n_iter=param_comb, scoring='roc_auc',\n",
    "                                           n_jobs=n_jobs,\n",
    "                                           cv=skf.split(X_train, Y_train),\n",
    "                                           verbose=3, random_state=42)\n",
    "\n",
    "        start_time = timer(None)\n",
    "        random_search.fit(X_train, Y_train)\n",
    "        timer(start_time)\n",
    "\n",
    "        print('--------------')\n",
    "        print('\\n all results:')\n",
    "        print(random_search.cv_results_)\n",
    "\n",
    "        print('\\n best estimator:')\n",
    "        print(random_search.best_estimator_)\n",
    "\n",
    "        print('\\n best normalized gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))\n",
    "        print(random_search.best_score_ * 2 - 1)\n",
    "\n",
    "        print('\\n best xgb hyperparameters:')\n",
    "        print(random_search.best_params_)\n",
    "\n",
    "        result_csv_path = 'xgb-random-grid-search-results.csv'\n",
    "        results = pd.DataFrame(random_search.cv_results_)\n",
    "        results.to_csv(result_csv_path, index=False)\n",
    "        print('save xgb random search results to {}'.format(result_csv_path))\n",
    "        print('--------------')\n",
    "\n",
    "        #xgb_clf = random_search\n",
    "        xgb_clf = random_search.best_estimator_\n",
    "    else:\n",
    "        xgb_clf.fit(X_train, Y_train)\n",
    "\n",
    "    if model_path is None:\n",
    "        xgb_model_path = 'xgb.model'\n",
    "        if hyperparameter_tuning:\n",
    "            xgb_model_path = 'xgb.ht.model'\n",
    "    else:\n",
    "        xgb_model_path = model_path\n",
    "        # xgb_clf.save_model(xgb_model_path)\n",
    "    joblib.dump(xgb_clf, xgb_model_path)\n",
    "    print('\\n save the xgb model to {}'.format(xgb_model_path))\n",
    "\n",
    "    return xgb_clf, xgb_model_path\n",
    "\n",
    "\n",
    "def train_rf(X_train, Y_train, hyperparameter_tuning=False, model_path=None, n_jobs=3, folds=3, n_estimators=100):\n",
    "    \"\"\"\n",
    "    train a RF classifier\n",
    "\n",
    "    Reference\n",
    "    https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "    \"\"\"\n",
    "    print('\\n === train a random forest model === \\n')\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=n_estimators, random_state=42, n_jobs=n_jobs)\n",
    "\n",
    "    if hyperparameter_tuning:\n",
    "        # Number of trees in random forest\n",
    "        #n_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]\n",
    "        n_estimators = [5, 10, 80, 100, 200]\n",
    "        # Number of features to consider at every split\n",
    "        max_features = ['auto', 'sqrt']\n",
    "        # Maximum number of levels in tree\n",
    "        #max_depth = [int(x) for x in np.linspace(10, 110, num=11)]\n",
    "        max_depth = [4, 6, 8]\n",
    "        max_depth.append(None)\n",
    "        # Minimum number of samples required to split a node\n",
    "        min_samples_split = [2, 5, 10]\n",
    "        # Minimum number of samples required at each leaf node\n",
    "        min_samples_leaf = [1, 2, 4]\n",
    "        # Method of selecting samples for training each tree\n",
    "        bootstrap = [True, False]\n",
    "        # Create the random grid\n",
    "        random_grid = {'n_estimators': n_estimators,\n",
    "                       'max_features': max_features,\n",
    "                       'max_depth': max_depth,\n",
    "                       'min_samples_split': min_samples_split,\n",
    "                       'min_samples_leaf': min_samples_leaf,\n",
    "                       'bootstrap': bootstrap}\n",
    "        #print(random_grid)\n",
    "\n",
    "        rf_random = RandomizedSearchCV(estimator=model, param_distributions=random_grid,\n",
    "                                       n_iter=100, cv=folds, verbose=2, random_state=42, n_jobs=n_jobs)\n",
    "\n",
    "        rf_random.fit(X_train, X_train)\n",
    "\n",
    "\n",
    "        print('--------------')\n",
    "        print('\\n all results:')\n",
    "        print(rf_random.cv_results_)\n",
    "\n",
    "        print('\\n best estimator:')\n",
    "        print(rf_random.best_estimator_)\n",
    "\n",
    "        print('\\n best rf parameters:')\n",
    "        print(rf_random.best_params_)\n",
    "\n",
    "        print('\\n best scores:')\n",
    "        rf_random.best_score_\n",
    "\n",
    "        result_cv_path = 'rf-random-grid-search-results.csv'\n",
    "        results = pd.DataFrame(rf_random.cv_results_)\n",
    "        results.to_csv(result_cv_path, index=False)\n",
    "        print('\\n save rf random search results to {}'.format(result_cv_path))\n",
    "        print('--------------')\n",
    "\n",
    "        model = rf_random.best_estimator_\n",
    "    else:\n",
    "        model.fit(X_train, Y_train)\n",
    "\n",
    "    if model_path is None:\n",
    "        model_path = 'rf.model'\n",
    "        if hyperparameter_tuning:\n",
    "            model_path = 'rf.ht.model'\n",
    "\n",
    "\n",
    "    joblib.dump(model, model_path)\n",
    "    print('\\n save the rf model to {}'.format(model_path))\n",
    "\n",
    "    return model, model_path\n",
    "\n",
    "\n",
    "def train_nb(X_train, Y_train, model_path=None):\n",
    "    \"\"\"\n",
    "    train a naive bayes classifier\n",
    "    \"\"\"\n",
    "    # reference https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/\n",
    "\n",
    "    print('\\n === train a gaussian naive bayes model === \\n')\n",
    "\n",
    "    model = GaussianNB()\n",
    "    model.fit(X_train, Y_train,)\n",
    "\n",
    "    if model_path is None:\n",
    "        model_path = 'nb.model'\n",
    "\n",
    "    joblib.dump(model, model_path)\n",
    "    print('\\n save the GaussianNB model to {}'.format(model_path))\n",
    "\n",
    "    return model, model_path\n",
    "\n",
    "\n",
    "def train_lgbm(X_train, Y_train,\n",
    "               # categorical_feature=['referer_code', 'is_app', 'agent_id', 'traffic_type', 'action_id', 'reference'],\n",
    "               categorical_feature='auto',\n",
    "               model_path=None, n_jobs=3, hyperparameter_tuning=False, num_boost_round=100, folds=3):\n",
    "    \"\"\"\n",
    "    train a lightGBM model\n",
    "\n",
    "    Reference\n",
    "    https://github.com/Microsoft/LightGBM/blob/master/examples/python-guide/advanced_example.py\n",
    "    https://www.kaggle.com/garethjns/microsoft-lightgbm-with-parameter-tuning-0-823?scriptVersionId=1751960\n",
    "    \"\"\"\n",
    "\n",
    "    print('\\n === train a lightGBM === \\n')\n",
    "\n",
    "    d_train = lgb.Dataset(X_train, label=Y_train,\n",
    "                          # categorical_feature=['aisle_id', 'department_id']\n",
    "                          categorical_feature=categorical_feature,\n",
    "                          )\n",
    "\n",
    "\n",
    "    if not hyperparameter_tuning:\n",
    "        # params = {\n",
    "        #     'boosting_type': 'gbdt',\n",
    "        #     'objective': 'binary',\n",
    "        #     'num_class': 1,                # must be 1 for non-multiclass training\n",
    "        #     'metric': 'binary_error',\n",
    "        #     #'metric': 'binary_logloss',\n",
    "        #     #'n_jobs': n_jobs,\n",
    "        #     'nthread': n_jobs,\n",
    "        #     #'num_leaves': 31,\n",
    "        #\n",
    "        #     'num_leaves': 64,\n",
    "        #     'min_child_weight': 1,\n",
    "        #     'min_child_samples': 5,\n",
    "        #     'scale_pos_weight': 1,\n",
    "        #     'reg_alpha': 5,\n",
    "        #     'learning_rate': 0.05,\n",
    "        #     'max_bin': 512,\n",
    "        #\n",
    "        #     #'feature_fraction': 0.9,\n",
    "        #     #'bagging_fraction': 0.8,\n",
    "        #     #'bagging_freq': 5,\n",
    "        #     #'verbose': 0\n",
    "        # }\n",
    "\n",
    "        params = {'boosting_type': 'gbdt',\n",
    "                  'max_depth': -1,\n",
    "                  'objective': 'binary',\n",
    "                  'nthread': n_jobs,  # Updated from nthread\n",
    "                  'num_leaves': 64,\n",
    "                  'learning_rate': 0.05,\n",
    "                  'max_bin': 512,\n",
    "                  'subsample_for_bin': 200,\n",
    "                  'subsample': 1,\n",
    "                  'subsample_freq': 1,\n",
    "                  'colsample_bytree': 0.8,\n",
    "                  'reg_alpha': 5,\n",
    "                  'reg_lambda': 10,\n",
    "                  'min_split_gain': 0.5,\n",
    "                  'min_child_weight': 1,\n",
    "                  'min_child_samples': 5,\n",
    "                  'scale_pos_weight': 1,\n",
    "                  'num_class': 1,\n",
    "                  'metric': 'binary_error'}\n",
    "\n",
    "        gbm = lgb.train(params,\n",
    "                        d_train,\n",
    "                        num_boost_round=num_boost_round,\n",
    "                        categorical_feature=categorical_feature)\n",
    "\n",
    "    else:\n",
    "        params = {'boosting_type': 'gbdt',\n",
    "                  'max_depth': -1,\n",
    "                  'objective': 'binary',\n",
    "                  'nthread': n_jobs,  # Updated from nthread\n",
    "                  'num_leaves': 64,\n",
    "                  'learning_rate': 0.05,\n",
    "                  'max_bin': 512,\n",
    "                  'subsample_for_bin': 200,\n",
    "                  'subsample': 1,\n",
    "                  'subsample_freq': 1,\n",
    "                  'colsample_bytree': 0.8,\n",
    "                  'reg_alpha': 5,\n",
    "                  'reg_lambda': 10,\n",
    "                  'min_split_gain': 0.5,\n",
    "                  'min_child_weight': 1,\n",
    "                  'min_child_samples': 5,\n",
    "                  'scale_pos_weight': 1,\n",
    "                  'num_class': 1,\n",
    "                  'metric': 'binary_error'}\n",
    "\n",
    "        gridParams = {\n",
    "            'learning_rate': [0.005],\n",
    "            'n_estimators': [8, 16, 24],\n",
    "            'num_leaves': [6, 8, 12, 16],\n",
    "            'boosting_type': ['gbdt'],\n",
    "            'objective': ['binary'],\n",
    "            'random_state': [42],  # Updated from 'seed'\n",
    "            'colsample_bytree': [0.64, 0.65, 0.66],\n",
    "            'subsample': [0.7, 0.75],\n",
    "            'reg_alpha': [1, 1.2],\n",
    "            'reg_lambda': [1, 1.2, 1.4],\n",
    "        }\n",
    "\n",
    "        mdl = lgb.LGBMClassifier(boosting_type='gbdt',\n",
    "                                 objective='binary',\n",
    "                                 n_jobs=n_jobs,  # Updated from 'nthread'\n",
    "                                 silent=True,\n",
    "                                 max_depth=params['max_depth'],\n",
    "                                 max_bin=params['max_bin'],\n",
    "                                 subsample_for_bin=params['subsample_for_bin'],\n",
    "                                 subsample=params['subsample'],\n",
    "                                 subsample_freq=params['subsample_freq'],\n",
    "                                 min_split_gain=params['min_split_gain'],\n",
    "                                 min_child_weight=params['min_child_weight'],\n",
    "                                 min_child_samples=params['min_child_samples'],\n",
    "                                 scale_pos_weight=params['scale_pos_weight'])\n",
    "\n",
    "        print(mdl.get_params().keys())\n",
    "\n",
    "        grid = RandomizedSearchCV(estimator=mdl, param_distributions=gridParams,\n",
    "                                       n_iter=100, cv=folds, verbose=2, random_state=42, n_jobs=n_jobs)\n",
    "\n",
    "        #grid = GridSearchCV(mdl, gridParams, verbose=2, cv=folds, n_jobs=n_jobs)\n",
    "        grid.fit(X_train, Y_train)\n",
    "\n",
    "        print('best parameters:')\n",
    "        print(grid.best_params_)\n",
    "        print('best score: ')\n",
    "        print(grid.best_score_)\n",
    "\n",
    "        # using parameters already set above, replace in the best from the grid search\n",
    "        params['colsample_bytree'] = grid.best_params_['colsample_bytree']\n",
    "        params['learning_rate'] = grid.best_params_['learning_rate']\n",
    "        #params['max_bin'] = grid.best_params_['max_bin']\n",
    "        params['num_leaves'] = grid.best_params_['num_leaves']\n",
    "        params['reg_alpha'] = grid.best_params_['reg_alpha']\n",
    "        params['reg_lambda'] = grid.best_params_['reg_lambda']\n",
    "        params['subsample'] = grid.best_params_['subsample']\n",
    "        #params['subsample_for_bin'] = grid.best_params_['subsample_for_bin']\n",
    "\n",
    "        print('Fitting with params: ')\n",
    "        print(params)\n",
    "\n",
    "        X_train_sub, X_val, Y_train_sub, Y_val = train_test_split(X_train, Y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "        d_train_sub = lgb.Dataset(X_train_sub, label=Y_train_sub,\n",
    "                              # categorical_feature=['aisle_id', 'department_id']\n",
    "                              categorical_feature=categorical_feature,\n",
    "                              #categorical_feature='auto'\n",
    "                              )\n",
    "\n",
    "        d_val_sub = lgb.Dataset(X_val, label=Y_val,\n",
    "                              # categorical_feature=['aisle_id', 'department_id']\n",
    "                              categorical_feature=categorical_feature,\n",
    "                              #categorical_feature='auto'\n",
    "                              )\n",
    "\n",
    "        gbm = lgb.train(params,\n",
    "                        d_train_sub,\n",
    "                        num_boost_round=1000,\n",
    "                        valid_sets=[d_train_sub, d_val_sub],\n",
    "                        early_stopping_rounds=50,\n",
    "                        verbose_eval=4)\n",
    "\n",
    "        # Plot importance\n",
    "        #lgb.plot_importance(gbm)\n",
    "\n",
    "    if model_path is None:\n",
    "        model_path = 'lgbm.model'\n",
    "        if hyperparameter_tuning:\n",
    "            model_path = 'lgbm.ht.model'\n",
    "\n",
    "    # save model to file\n",
    "    gbm.save_model(model_path)\n",
    "    print('save the lightGBM model to {}'.format(model_path))\n",
    "\n",
    "    # load model to predict\n",
    "    # print('Load model to predict')\n",
    "    # bst = lgb.Booster(model_file='model.txt')\n",
    "    # can only predict with the best iteration (or the saving iteration)\n",
    "    # y_pred = bst.predict(X_test)\n",
    "\n",
    "    return gbm, model_path\n",
    "\n",
    "\n",
    "def train_catboost(X_train, Y_train,\n",
    "                   categorical_feature=[0, 1, 2, 3, 4, 5],\n",
    "                   #categorical_feature=['referer_code', 'is_app', 'agent_id', 'traffic_type', 'action_id', 'reference'],\n",
    "                   model_path=None, hyperparameter_tuning=False, num_boost_round=100):\n",
    "    \"\"\"\n",
    "    train a catboost model\n",
    "\n",
    "    Reference:\n",
    "    https://tech.yandex.com/catboost/doc/dg/concepts/python-usages-examples-docpage/\n",
    "    \"\"\"\n",
    "\n",
    "    print('\\n === train a catboost === \\n')\n",
    "\n",
    "    model = CatBoostClassifier(loss_function='Logloss',\n",
    "                               iterations=num_boost_round,\n",
    "                               #learning_rate=1,\n",
    "                               #depth=2\n",
    "                               )\n",
    "    model.fit(X_train, Y_train, categorical_feature)\n",
    "\n",
    "    if model_path is None:\n",
    "        model_path = 'catboost.model'\n",
    "        if hyperparameter_tuning:\n",
    "            model_path = 'catboost.ht.model'\n",
    "\n",
    "    model.save_model(model_path)\n",
    "\n",
    "    print('\\n save the catboost model to {}'.format(model_path))\n",
    "\n",
    "    return model, model_path\n",
    "\n",
    "def train_cnn(X_train, Y_train, model_path, maxlen = 400, epochs = 2):\n",
    "    # max_features = 5000\n",
    "    max_features = 1000\n",
    "    batch_size = 64\n",
    "    embedding_dims = 20\n",
    "    filters = 250\n",
    "    kernel_size = 3\n",
    "    hidden_dims = 100\n",
    "\n",
    "    # X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "    # Y_train = to_categorical(Y_train)\n",
    "\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features,\n",
    "                        embedding_dims,\n",
    "                        input_length=maxlen))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # we add a Convolution1D, which will learn filters\n",
    "    # word group filters of size filter_length:\n",
    "    model.add(Conv1D(filters,\n",
    "                     kernel_size,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "    # we use max pooling:\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "\n",
    "    # We add a vanilla hidden layer:\n",
    "    model.add(Dense(hidden_dims))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_train, Y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              # validation_data=(x_test, y_test),\n",
    "              verbose=2,\n",
    "              )\n",
    "\n",
    "    model.save(model_path)\n",
    "\n",
    "    return model, model_path\n",
    "\n",
    "\n",
    "def predict(model_path, X_test, is_lgbm=False, is_catboost=False, is_cnn=False, maxlen=400, lgbm_threshold=0.5):\n",
    "    \"\"\"\n",
    "    load the model and predict unseen data\n",
    "    \"\"\"\n",
    "\n",
    "    print('\\n === predict === \\n')\n",
    "\n",
    "    if is_lgbm:\n",
    "        # lightgbm\n",
    "        model = lgb.Booster(model_file=model_path)\n",
    "    elif is_catboost:\n",
    "        model = CatBoostClassifier()\n",
    "        model = model.load_model(model_path)\n",
    "    elif is_cnn:\n",
    "        model = load_model(model_path)\n",
    "    else:\n",
    "        # sklearn\n",
    "        # xgboost\n",
    "        model = joblib.load(model_path)\n",
    "\n",
    "    # y_pred = model.predict_prob(X_test)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    if is_lgbm:\n",
    "        #print('==')\n",
    "        #print(y_pred)\n",
    "        y_output = []\n",
    "        for y in y_pred:\n",
    "            if y > lgbm_threshold:\n",
    "                y_output.append(1)\n",
    "            else:\n",
    "                y_output.append(0)\n",
    "        #print('==')\n",
    "        #print(y_output)\n",
    "        return(np.array(y_output))\n",
    "        #return np.array([np.argmax(y) for y in y_pred])\n",
    "    elif is_cnn:\n",
    "        # X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred = [np.argmax(y) for y in y_pred]\n",
    "        return np.array(y_pred)\n",
    "    else:\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "def blend_predictions(y_pred_list, threshold=0.5):\n",
    "    \"\"\"\n",
    "    blend the predictions\n",
    "    \"\"\"\n",
    "\n",
    "    print('\\n === blend predictions === \\n')\n",
    "\n",
    "    y_pred = y_pred_list[0]\n",
    "\n",
    "    for i in range(1, len(y_pred_list)):\n",
    "        for j in range(len(y_pred)):\n",
    "            y_pred[j] += y_pred_list[i][j]\n",
    "\n",
    "    # average the predictions\n",
    "    y_pred = y_pred*1.0 / len(y_pred_list)\n",
    "\n",
    "    y_output = []\n",
    "    for y in y_pred:\n",
    "        if y > threshold:\n",
    "            y_output.append(1)\n",
    "        else:\n",
    "            y_output.append(0)\n",
    "\n",
    "    return np.array(y_output)\n",
    "\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    evaluate the prediction\n",
    "    \"\"\"\n",
    "    print('\\n === evaluate === \\n')\n",
    "\n",
    "    nb_bookings_true = 0\n",
    "    for y in y_true:\n",
    "        if y == 1:\n",
    "            nb_bookings_true += 1\n",
    "    print('\\n number of bookings in y_true: {}'.format(nb_bookings_true))\n",
    "    print('\\n y_true shape:')\n",
    "    print(y_true.shape)\n",
    "\n",
    "    nb_bookings_pred = 0\n",
    "    for y in y_pred:\n",
    "        if y == 1:\n",
    "            nb_bookings_pred += 1\n",
    "    print('\\n number of bookings in y_pred {}'.format(nb_bookings_pred))\n",
    "    print('\\n y_pred shape:')\n",
    "    print(y_pred.shape)\n",
    "\n",
    "    mcc_score = matthews_corrcoef(y_true, y_pred)\n",
    "    print('\\n matthews corrcoef score {}'.format(mcc_score))\n",
    "\n",
    "    \"\"\"\n",
    "      tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "      print(tn)\n",
    "      print(fp)\n",
    "      print(fn)\n",
    "      print(tp)\n",
    "      print('---')\n",
    "\n",
    "      mcc = (tp*tn - fp*fn) / np.math.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "      print(mcc)\n",
    "      print(matthews_corrcoef(y_true, y_pred))\n",
    "      \"\"\"\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print('\\n accuracy: {}'.format(accuracy))\n",
    "    print('\\n classification report:')\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "    f1score = f1_score(y_true, y_pred, average='binary')\n",
    "    print('\\n f1 score: {}'.format(f1score))\n",
    "\n",
    "    #roc_auc_score(y_true, y_pred)\n",
    "\n",
    "    return mcc_score, accuracy, f1score\n",
    "\n",
    "\n",
    "def plot_performance(x_list, y_list, title, x_label, y_label):\n",
    "    \"\"\"\n",
    "    define the plotting function\n",
    "    \"\"\"\n",
    "    plt.plot(x_list, y_list)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def save_prediction(target_user_df, y_pred, file_path):\n",
    "    \"\"\"\n",
    "    save predictions to a csv file\n",
    "    \"\"\"\n",
    "    prediciton_df = pd.DataFrame(columns=['session_id', 'has_booking'])\n",
    "    prediciton_df['session_id'] = target_user_df['session_id'].values\n",
    "    prediciton_df['has_booking'] = [int(y) for y in y_pred]\n",
    "\n",
    "    print('prediction:')\n",
    "    print(prediciton_df['has_booking'].unique())\n",
    "\n",
    "    prediciton_df.to_csv(file_path, '\\t', index=False)\n",
    "    print('save prediction to {}'.format(file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model_name, train_sub_x, val_x, train_sub_y, val_y, nb_prev_step, num_boost_rounds, hyperparameter_tuning,\n",
    "               category_feature_index_list,):\n",
    "    \"\"\"\n",
    "    define the model evaluation function\n",
    "    \"\"\"\n",
    "    \n",
    "    mcc_list = []\n",
    "    acc_list = []\n",
    "    f1_list  = []\n",
    "    dict_mcc_score = dict()\n",
    "        \n",
    "    for num_boost_round in num_boost_rounds:\n",
    "        print('\\nnum boost round: {}'.format(num_boost_round))\n",
    "\n",
    "        y_pred = None\n",
    "\n",
    "        if model_name == 'catboost':\n",
    "            model_path = 'catboost-[num_boost_round]{}-[ht]{}-[nb_prev]{}-sub.model'.format(num_boost_round,\n",
    "                                                                                             hyperparameter_tuning,\n",
    "                                                                                             nb_prev_step)\n",
    "\n",
    "            model, model_path = train_catboost(train_sub_x, train_sub_y, hyperparameter_tuning=hyperparameter_tuning,\n",
    "                                               categorical_feature=category_feature_index_list,\n",
    "                                               model_path=model_path, num_boost_round=num_boost_round)\n",
    "            y_pred = predict(model_path, val_x, is_catboost=True)\n",
    "            \n",
    "        elif model_name == 'xgb':\n",
    "            model_path = 'xgb-[num_boost_round]{}-[ht]{}-[nb_prev]{}-sub.model'.format(num_boost_round,\n",
    "                                                                                       hyperparameter_tuning,\n",
    "                                                                                       nb_prev_step)\n",
    "            model, model_path = train_xgb(X_train=train_sub_x, Y_train=train_sub_y,\n",
    "                                          hyperparameter_tuning=hyperparameter_tuning,\n",
    "                                          model_path=model_path, n_estimators=num_boost_round)\n",
    "            y_pred = predict(model_path, val_x)\n",
    "\n",
    "        elif model_name == 'lgbm':\n",
    "            model_path = 'lgbm-[num_boost_round]{}-[ht]{}-[nb_prev]{}-sub.model'.format(num_boost_round,\n",
    "                                                                                       hyperparameter_tuning,\n",
    "                                                                                       nb_prev_step)\n",
    "\n",
    "            model, model_path = train_lgbm(train_sub_x, train_sub_y, hyperparameter_tuning=hyperparameter_tuning,\n",
    "                                           categorical_feature=categorical_feature,\n",
    "                                           model_path=model_path, num_boost_round=num_boost_round)\n",
    "            \n",
    "            y_pred = predict(model_path, val_x, is_lgbm=True)\n",
    "        \n",
    "        elif model_name == 'rf':\n",
    "            model_path = 'rf-[num_boost_round]{}-[ht]{}-[nb_prev]{}-sub.model'.format(num_boost_round,\n",
    "                                                                                      hyperparameter_tuning,\n",
    "                                                                                      nb_prev_step)\n",
    "            \n",
    "            model, model_path = train_rf(X_train=train_sub_x, Y_train=train_sub_y, \n",
    "                                         hyperparameter_tuning=hyperparameter_tuning,\n",
    "                                         model_path=model_path, n_jobs=2, n_estimators=num_boost_round)\n",
    "            \n",
    "            y_pred = predict(model_path, val_x)\n",
    "            \n",
    "        elif model_name == 'cnn':\n",
    "            epochs = 100\n",
    "            model_path = 'cnn-[epochs]{}-[nb_prev]{}-sub.model'.format(epochs, nb_prev_step)\n",
    "            maxlen = train_sub_x.shape[1]\n",
    "            model, model_path = train_cnn(train_sub_x, train_sub_y,\n",
    "                                           model_path=model_path, epochs=epochs, maxlen=maxlen)\n",
    "            y_pred = predict(model_path, val_x, is_cnn=True, maxlen=maxlen)\n",
    "\n",
    "        mcc_score, accuracy, f1score = evaluate(val_y, y_pred)\n",
    "        \n",
    "        dict_mcc_score[num_boost_round] = mcc_score\n",
    "        \n",
    "        mcc_list.append(mcc_score)\n",
    "        acc_list.append(accuracy)\n",
    "        f1_list.append(f1score)\n",
    "\n",
    "    plot_performance(num_boost_rounds, mcc_list, 'mcc score (model: {} #steps: {})'.format(model_name, nb_prev_step), \n",
    "                     'num_boost_round', 'mcc score')\n",
    "    \n",
    "    plot_performance(num_boost_rounds, acc_list, 'accuracy (model: {} #steps: {})'.format(model_name, nb_prev_step), \n",
    "                     'num_boost_round', 'accuracy')\n",
    "    \n",
    "    plot_performance(num_boost_rounds, f1_list, 'f1 score (model: {} #steps: {})'.format(model_name, nb_prev_step), \n",
    "                     'num_boost_round', 'f1 score')\n",
    "    \n",
    "    return dict_mcc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature columns: \n",
      "['referer_code', 'is_app', 'agent_id', 'traffic_type', 'action_id', 'reference', 'step', 'action_id_1', 'reference_1', 'action_id_2', 'reference_2', 'action_id_3', 'reference_3', 'action_id_4', 'reference_4', 'action_id_5', 'reference_5', 'action_id_6', 'reference_6', 'action_id_7', 'reference_7', 'action_id_8', 'reference_8', 'action_id_9', 'reference_9', 'action_id_10', 'reference_10', 'action_id_11', 'reference_11', 'action_id_12', 'reference_12', 'action_id_13', 'reference_13', 'action_id_14', 'reference_14', 'action_id_15', 'reference_15', 'action_id_16', 'reference_16', 'action_id_17', 'reference_17', 'action_id_18', 'reference_18', 'action_id_19', 'reference_19', 'action_id_20', 'reference_20', 'action_id_21', 'reference_21', 'action_id_22', 'reference_22', 'action_id_23', 'reference_23', 'action_id_24', 'reference_24', 'action_id_25', 'reference_25', 'action_id_26', 'reference_26', 'action_id_27', 'reference_27', 'action_id_28', 'reference_28', 'action_id_29', 'reference_29', 'action_id_30', 'reference_30', 'action_id_31', 'reference_31', 'action_id_32', 'reference_32']\n",
      "category feature columns: \n",
      "['referer_code', 'is_app', 'agent_id', 'traffic_type', 'action_id', 'reference', 'action_id_1', 'reference_1', 'action_id_2', 'reference_2', 'action_id_3', 'reference_3', 'action_id_4', 'reference_4', 'action_id_5', 'reference_5', 'action_id_6', 'reference_6', 'action_id_7', 'reference_7', 'action_id_8', 'reference_8', 'action_id_9', 'reference_9', 'action_id_10', 'reference_10', 'action_id_11', 'reference_11', 'action_id_12', 'reference_12', 'action_id_13', 'reference_13', 'action_id_14', 'reference_14', 'action_id_15', 'reference_15', 'action_id_16', 'reference_16', 'action_id_17', 'reference_17', 'action_id_18', 'reference_18', 'action_id_19', 'reference_19', 'action_id_20', 'reference_20', 'action_id_21', 'reference_21', 'action_id_22', 'reference_22', 'action_id_23', 'reference_23', 'action_id_24', 'reference_24', 'action_id_25', 'reference_25', 'action_id_26', 'reference_26', 'action_id_27', 'reference_27', 'action_id_28', 'reference_28', 'action_id_29', 'reference_29', 'action_id_30', 'reference_30', 'action_id_31', 'reference_31', 'action_id_32', 'reference_32']\n",
      "category feature index list:\n",
      "[0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70]\n",
      "\n",
      " === get train set === \n",
      "\n",
      "\n",
      " -----\n",
      "train set size:\n",
      "(307677, 71)\n",
      "(307677,)\n",
      "feature columns: \n",
      "Index(['referer_code', 'is_app', 'agent_id', 'traffic_type', 'action_id',\n",
      "       'reference', 'step', 'action_id_1', 'reference_1', 'action_id_2',\n",
      "       'reference_2', 'action_id_3', 'reference_3', 'action_id_4',\n",
      "       'reference_4', 'action_id_5', 'reference_5', 'action_id_6',\n",
      "       'reference_6', 'action_id_7', 'reference_7', 'action_id_8',\n",
      "       'reference_8', 'action_id_9', 'reference_9', 'action_id_10',\n",
      "       'reference_10', 'action_id_11', 'reference_11', 'action_id_12',\n",
      "       'reference_12', 'action_id_13', 'reference_13', 'action_id_14',\n",
      "       'reference_14', 'action_id_15', 'reference_15', 'action_id_16',\n",
      "       'reference_16', 'action_id_17', 'reference_17', 'action_id_18',\n",
      "       'reference_18', 'action_id_19', 'reference_19', 'action_id_20',\n",
      "       'reference_20', 'action_id_21', 'reference_21', 'action_id_22',\n",
      "       'reference_22', 'action_id_23', 'reference_23', 'action_id_24',\n",
      "       'reference_24', 'action_id_25', 'reference_25', 'action_id_26',\n",
      "       'reference_26', 'action_id_27', 'reference_27', 'action_id_28',\n",
      "       'reference_28', 'action_id_29', 'reference_29', 'action_id_30',\n",
      "       'reference_30', 'action_id_31', 'reference_31', 'action_id_32',\n",
      "       'reference_32'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# --------------\n",
    "# Prepare the datasets\n",
    "\n",
    "nb_prev_step = 32\n",
    "\n",
    "no_feature_name_list = ['ymd', 'user_id', 'session_id', 'has_booking',]\n",
    "no_cat_feature_name  = ['step']\n",
    "\n",
    "target_columns       = ['has_booking']\n",
    "feature_columns      = ['referer_code', 'is_app', 'agent_id', 'traffic_type', 'action_id', 'reference', 'step']\n",
    "    \n",
    "    \n",
    "train_user_df = pd.read_csv('train_user_df-{}.csv'.format(nb_prev_step))\n",
    "target_user_df = pd.read_csv('target_user_df-{}.csv'.format(nb_prev_step))\n",
    "\n",
    "# get all features names\n",
    "feature_columns = [feature for feature in train_user_df.columns if not (feature in no_feature_name_list)]\n",
    "print('feature columns: ')\n",
    "print(feature_columns)\n",
    "\n",
    "# get all category features names\n",
    "categorical_feature = [feature for feature in feature_columns if (not feature in no_cat_feature_name)]\n",
    "print('category feature columns: ')\n",
    "print(categorical_feature)\n",
    "\n",
    "category_feature_index_list = [i for i, feature in enumerate(feature_columns) if not (feature in no_cat_feature_name)]\n",
    "print('category feature index list:')\n",
    "print(category_feature_index_list)\n",
    "\n",
    "# shuffle the train set\n",
    "train_user_df = train_user_df.reindex(np.random.permutation(train_user_df.index))\n",
    "\n",
    "train_x, train_y = get_train_set(train_user_df, feature_columns, target_columns)\n",
    "print('\\n -----')\n",
    "print('train set size:')\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "\n",
    "print('feature columns: ')\n",
    "print(train_x.columns)   \n",
    "\n",
    "# prepare validation set\n",
    "train_sub_x, val_x, train_sub_y, val_y = train_test_split(train_x, train_y, test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "num boost round: 100\n",
      "\n",
      " === train a xgb model === \n",
      "\n",
      "\n",
      " save the xgb model to xgb-[num_boost_round]100-[ht]False-[nb_prev]32-sub.model\n",
      "\n",
      " === predict === \n",
      "\n",
      "\n",
      " === evaluate === \n",
      "\n",
      "\n",
      " number of bookings in y_true: 2001\n",
      "\n",
      " y_true shape:\n",
      "(30768,)\n",
      "\n",
      " number of bookings in y_pred 67\n",
      "\n",
      " y_pred shape:\n",
      "(30768,)\n",
      "\n",
      " matthews corrcoef score 0.11774788466590541\n",
      "\n",
      " accuracy: 0.9357774310972439\n",
      "\n",
      " classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97     28767\n",
      "          1       0.69      0.02      0.04      2001\n",
      "\n",
      "avg / total       0.92      0.94      0.91     30768\n",
      "\n",
      "\n",
      " f1 score: 0.04448742746615087\n",
      "\n",
      "num boost round: 200\n",
      "\n",
      " === train a xgb model === \n",
      "\n",
      "\n",
      " save the xgb model to xgb-[num_boost_round]200-[ht]False-[nb_prev]32-sub.model\n",
      "\n",
      " === predict === \n",
      "\n",
      "\n",
      " === evaluate === \n",
      "\n",
      "\n",
      " number of bookings in y_true: 2001\n",
      "\n",
      " y_true shape:\n",
      "(30768,)\n",
      "\n",
      " number of bookings in y_pred 127\n",
      "\n",
      " y_pred shape:\n",
      "(30768,)\n",
      "\n",
      " matthews corrcoef score 0.17009575297938734\n",
      "\n",
      " accuracy: 0.9367524700988039\n",
      "\n",
      " classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97     28767\n",
      "          1       0.72      0.05      0.09      2001\n",
      "\n",
      "avg / total       0.92      0.94      0.91     30768\n",
      "\n",
      "\n",
      " f1 score: 0.08552631578947369\n",
      "\n",
      "num boost round: 500\n",
      "\n",
      " === train a xgb model === \n",
      "\n",
      "\n",
      " save the xgb model to xgb-[num_boost_round]500-[ht]False-[nb_prev]32-sub.model\n",
      "\n",
      " === predict === \n",
      "\n",
      "\n",
      " === evaluate === \n",
      "\n",
      "\n",
      " number of bookings in y_true: 2001\n",
      "\n",
      " y_true shape:\n",
      "(30768,)\n",
      "\n",
      " number of bookings in y_pred 245\n",
      "\n",
      " y_pred shape:\n",
      "(30768,)\n",
      "\n",
      " matthews corrcoef score 0.24775412850551082\n",
      "\n",
      " accuracy: 0.9388975559022361\n",
      "\n",
      " classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97     28767\n",
      "          1       0.75      0.09      0.16      2001\n",
      "\n",
      "avg / total       0.93      0.94      0.92     30768\n",
      "\n",
      "\n",
      " f1 score: 0.16295636687444345\n",
      "\n",
      "num boost round: 1000\n",
      "\n",
      " === train a xgb model === \n",
      "\n",
      "\n",
      " save the xgb model to xgb-[num_boost_round]1000-[ht]False-[nb_prev]32-sub.model\n",
      "\n",
      " === predict === \n",
      "\n",
      "\n",
      " === evaluate === \n",
      "\n",
      "\n",
      " number of bookings in y_true: 2001\n",
      "\n",
      " y_true shape:\n",
      "(30768,)\n",
      "\n",
      " number of bookings in y_pred 327\n",
      "\n",
      " y_pred shape:\n",
      "(30768,)\n",
      "\n",
      " matthews corrcoef score 0.2965758590795012\n",
      "\n",
      " accuracy: 0.9407176287051482\n",
      "\n",
      " classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97     28767\n",
      "          1       0.77      0.13      0.22      2001\n",
      "\n",
      "avg / total       0.93      0.94      0.92     30768\n",
      "\n",
      "\n",
      " f1 score: 0.21649484536082472\n",
      "\n",
      "num boost round: 1500\n",
      "\n",
      " === train a xgb model === \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------------\n",
    "# Plot the performance\n",
    "\n",
    "num_boost_rounds = [100, 200, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 6000, 10000]\n",
    "hyperparameter_tuning = False\n",
    "model_name = 'xgb'\n",
    "\n",
    "dict_mcc_score_xgb = evaluation(model_name, train_sub_x, val_x, train_sub_y, val_y, nb_prev_step, \n",
    "                            num_boost_rounds, hyperparameter_tuning, category_feature_index_list,)\n",
    "\n",
    "\n",
    "model_name = 'catboost'\n",
    "\n",
    "dict_mcc_score_catboost = evaluation(model_name, train_sub_x, val_x, train_sub_y, val_y, nb_prev_step, \n",
    "                            num_boost_rounds, hyperparameter_tuning, category_feature_index_list,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = get_test_set(target_user_df, feature_columns)\n",
    "\n",
    "print('\\n -----')\n",
    "print('test set size:')\n",
    "print(test_x.shape)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_boost_round = 1000\n",
    "\n",
    "model, model_path = train_catboost(train_x, train_y, hyperparameter_tuning=False,\n",
    "                                   categorical_feature=category_feature_index_list,\n",
    "                                   model_path=model_path, num_boost_round=num_boost_round)\n",
    "\n",
    "print('== make predictions (with historical data) ==')\n",
    "\n",
    "y_pred = predict(model_path=model_path, X_test=test_x, is_catboost=True)\n",
    "\n",
    "prediction_file_path = 'prediction-catboost-{}-{}-{}-{}.csv'.format(num_boost_round, nb_prev_step)\n",
    "\n",
    "save_prediction(target_user_df, y_pred, prediction_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The object of this task is to predict if a session has a booking nor not. This is a classic binary classification problem. The target is 0 or 1, where 0 means no booking and 1 means has a booking. The features I have used in this task are: session information (i.e., referer_code, is_app, agent_id, and traffic_type) and action information (i.e., action_id, reference) of the last n steps. Please note, ideally, for each session all the steps action information should be considered as features. Due to the limitation of the computation resource, I took the last n steps action information. \n",
    "\n",
    "We can see that in the experiments, with n=32, by using Gradient Boosting model, we achieved on the validation set. I believe with large n, the performance will be improved. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future work\n",
    "\n",
    "- Hyperparameter tuning with cross validation. Due to the limitation of time and computation power, I didn't try hyperparameter tuning for each of the models. However, the hyperparameter tuning functions have been implemented already.\n",
    "\n",
    "- Feature engineering, e.g., increase the number of last steps. Ideally, all the steps should be considered as features.\n",
    "\n",
    "- Deep learning for booking prediction. We could use RNN (or GRU, LSTM) to model the sequences of actions for the sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional questions\n",
    "\n",
    "### What makes the classification problem difficult in this task? How do you handle that?\n",
    "  \n",
    "  - One difficulty is that we have the categorical features with various values, such as action id, reference.\n",
    "  One solution is using the tree based models, such as xgboost, catboost, random forest, lightGBM. These model can handle the categorical features automatically. Another solution is using the on-hot encoding approach, by using prior knowledge, we can first group the values into several categories.\n",
    "  \n",
    "  - Another difficulty is a user will or will not book a hotel in the current step is not only affect by the current action but also affected by the previous actions. In other words, for each step, with only the current action information, it's not sufficient enough to predict if the user will book the hotel nor not. Therefore, the previous action information should be included as features. Note due to the limitation of time and computation power, only a part of the experiment results are included in this notebook. The details are explained in Section \"Second approach: Using historical data\".\n",
    "  \n",
    "\n",
    "### Evaluate and compare at least 3 classification methods for this task.\n",
    "This has been shown in Sections \"First approach\" and \"Second approach: Using historical data\".\n",
    "  \n",
    "\n",
    "### Propose at least 3 features that are significant to predict bookings?\n",
    "\n",
    "* User historical information\n",
    "  For example,\n",
    "    - The historical booking information\n",
    "    - The historical searching information (this feature I have used in previous section of this notebook)\n",
    "    \n",
    "* Hotel information\n",
    "  If a user select a hotel (e.g., click the button 'View Deal') then the features of the hotel is significant to predict bookings. For example:\n",
    "    - Images\n",
    "    - Reviews\n",
    "    - Rating\n",
    "    - Location\n",
    "    - Discount\n",
    "    - Number of users who viewed this hotel\n",
    "    \n",
    "* Time information\n",
    "  The time information could be also helpful. For example, we could convert the dates into several categories. Because we know that during the tourist season, the number of bookings is large.\n",
    "   \n",
    "* Action information\n",
    "  If a user select a hotel, we can provide a button 'Add to list'. In other words, if a user adds some hotels to the list, the likelihood for him to book a hotel will be high. However, I have checked several hotel/flight booking engines. None of them have this feature. Therefore, I doubt the effectiveness of this feature. Personally I think this feature is useful. But I don't know the reason why this feature is not there.\n",
    "  \n",
    "\n",
    "### We can spot a very significant action type. What might this action refer to?\n",
    "\n",
    "It has been shown in Section \"Step 1: Step 1: read and explore the data\" that action 2142 has the largest number of bookings, i.e., 173073. This action might refer to clicking the 'View Deal'."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
